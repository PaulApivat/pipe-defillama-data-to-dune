"""
Unit Tests for Pipeline Methods

Tests specific pipeline methods to avoid previous issues:
- GitHub Actions workflow integration
- Historical facts production validation
- Daily update date validation
- Date redundancy prevention
"""

import os
import sys
from datetime import date, timedelta
from unittest.mock import Mock, patch, MagicMock
import polars as pl
import logging

# Add src to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.orchestration.pipeline import PipelineOrchestrator
from src.transformation.schemas import HISTORICAL_FACTS_SCHEMA
from src.transformation.transformers import create_historical_facts

# Setup logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def test_github_actions_initial_load_workflow():
    """
    Test that defillama_initial_load.yml workflow calls run_initial_load()

    This test validates the GitHub Actions workflow integration
    """
    print("üß™ Testing GitHub Actions Initial Load Workflow Integration")
    print("=" * 60)

    try:
        # Create pipeline orchestrator in dry-run mode
        pipeline = PipelineOrchestrator(dry_run=True)

        # Mock the run_initial_load method to track calls
        pipeline.run_initial_load = Mock(return_value=True)

        # Simulate GitHub Actions workflow calling the pipeline
        # This mimics: python -m src.orchestration.pipeline --mode initial --verbose
        success = pipeline.run_initial_load()

        # Assert that run_initial_load was called
        pipeline.run_initial_load.assert_called_once()

        # Assert that the call returned success
        assert (
            success == True
        ), "run_initial_load() should return True for successful execution"

        print("‚úÖ GitHub Actions initial load workflow integration test passed")
        print("   - run_initial_load() method is callable from workflow")
        print("   - Method returns success status")

        return True

    except Exception as e:
        print(f"‚ùå GitHub Actions initial load workflow test failed: {e}")
        return False


def test_run_initial_load_produces_historical_facts():
    """
    Test that run_initial_load() produces historical facts with correct schema

    This test validates that the initial load actually creates the expected data structure
    """
    print("\nüß™ Testing run_initial_load() Produces Historical Facts")
    print("=" * 60)

    try:
        # Create pipeline orchestrator in dry-run mode
        pipeline = PipelineOrchestrator(dry_run=True)

        # Mock the data fetching and transformation methods
        with patch(
            "src.orchestration.pipeline.fetch_raw_pools_data"
        ) as mock_pools, patch(
            "src.orchestration.pipeline.fetch_raw_tvl_data"
        ) as mock_tvl, patch(
            "src.orchestration.pipeline.create_pool_dimensions"
        ) as mock_dims, patch(
            "src.orchestration.pipeline.filter_pools_by_projects"
        ) as mock_filter, patch(
            "src.orchestration.pipeline.create_historical_facts"
        ) as mock_facts, patch(
            "src.orchestration.pipeline.save_transformed_data"
        ) as mock_save:

            # Create mock data with consistent column lengths
            mock_pools_df = pl.DataFrame(
                {
                    "pool_id": ["test-pool-1", "test-pool-2"],
                    "protocol_slug": ["test-protocol", "test-protocol"],
                    "chain": ["ethereum", "ethereum"],
                    "symbol": ["TEST", "TEST"],
                }
            )

            mock_tvl_df = pl.DataFrame(
                {
                    "pool_id": ["test-pool-1", "test-pool-2"],
                    "timestamp": [date.today(), date.today()],
                    "tvl_usd": [1000.0, 2000.0],
                    "apy": [5.0, 6.0],
                    "apy_base": [2.0, 3.0],
                    "apy_reward": [3.0, 3.0],
                }
            )

            mock_dimensions_df = pl.DataFrame(
                {
                    "pool_id": ["test-pool-1", "test-pool-2"],
                    "pool_old": ["test-pool-1", "test-pool-2"],
                    "protocol_slug": ["test-protocol", "test-protocol"],
                    "chain": ["ethereum", "ethereum"],
                    "symbol": ["TEST", "TEST"],
                }
            )

            # Create mock historical facts with correct schema
            mock_historical_facts = pl.DataFrame(
                {
                    "timestamp": [date.today(), date.today()],
                    "pool_id": [b"0x1234567890abcdef", b"0xabcdef1234567890"],
                    "pool_id_defillama": ["test-pool-1", "test-pool-2"],
                    "protocol_slug": ["test-protocol", "test-protocol"],
                    "chain": ["ethereum", "ethereum"],
                    "symbol": ["TEST", "TEST"],
                    "tvl_usd": [1000.0, 2000.0],
                    "apy": [5.0, 6.0],
                    "apy_base": [2.0, 3.0],
                    "apy_reward": [3.0, 3.0],
                },
                schema=HISTORICAL_FACTS_SCHEMA,
            )

            # Configure mocks
            mock_pools.return_value = mock_pools_df
            mock_tvl.return_value = mock_tvl_df
            mock_dims.return_value = mock_dimensions_df
            mock_filter.return_value = mock_dimensions_df
            mock_facts.return_value = mock_historical_facts
            mock_save.return_value = None

            # Run initial load
            success = pipeline.run_initial_load()

            # Assertions
            assert success == True, "run_initial_load() should return True"

            # Verify that create_historical_facts was called
            mock_facts.assert_called_once()

            # Verify that the historical facts have the correct schema
            historical_facts_call = mock_facts.call_args[0]
            historical_facts_df = historical_facts_call[0]  # First argument

            # Check schema matches HISTORICAL_FACTS_SCHEMA
            assert (
                historical_facts_df.schema == HISTORICAL_FACTS_SCHEMA
            ), f"Historical facts schema mismatch. Expected: {HISTORICAL_FACTS_SCHEMA}, Got: {historical_facts_df.schema}"

            # Check that it has the expected columns
            expected_columns = [
                "timestamp",
                "pool_id",
                "pool_id_defillama",
                "protocol_slug",
                "chain",
                "symbol",
                "tvl_usd",
                "apy",
                "apy_base",
                "apy_reward",
            ]
            assert set(historical_facts_df.columns) == set(
                expected_columns
            ), f"Missing columns. Expected: {expected_columns}, Got: {historical_facts_df.columns}"

            # Check data types
            assert (
                historical_facts_df.schema["pool_id"] == pl.Binary()
            ), "pool_id should be Binary type"
            assert (
                historical_facts_df.schema["pool_id_defillama"] == pl.String()
            ), "pool_id_defillama should be String type"
            assert (
                historical_facts_df.schema["tvl_usd"] == pl.Float64()
            ), "tvl_usd should be Float64 type"

            print("‚úÖ run_initial_load() produces historical facts test passed")
            print("   - Historical facts DataFrame created with correct schema")
            print("   - All required columns present")
            print("   - Data types match HISTORICAL_FACTS_SCHEMA")
            print(f"   - Sample data: {historical_facts_df.height} records")

            return True

    except Exception as e:
        print(f"‚ùå run_initial_load() produces historical facts test failed: {e}")
        return False


def test_run_daily_update_requires_target_date():
    """
    Test that run_daily_update() requires target_date parameter

    This test validates that target_date is now required for daily updates
    """
    print("\nüß™ Testing run_daily_update() Target Date Requirement")
    print("=" * 60)

    try:
        # Create pipeline orchestrator in dry-run mode
        pipeline = PipelineOrchestrator(dry_run=True)

        # Test 1: Should work with explicit target_date
        test_date = date.today() - timedelta(days=1)

        with patch(
            "src.orchestration.pipeline.fetch_raw_pools_data"
        ) as mock_pools, patch(
            "src.orchestration.pipeline.fetch_raw_tvl_data"
        ) as mock_tvl, patch(
            "src.orchestration.pipeline.create_pool_dimensions"
        ) as mock_dims, patch(
            "src.orchestration.pipeline.filter_pools_by_projects"
        ) as mock_filter, patch(
            "src.orchestration.pipeline.create_historical_facts"
        ) as mock_facts, patch(
            "src.orchestration.pipeline.save_transformed_data"
        ) as mock_save:

            # Configure mocks with proper schema
            mock_pools.return_value = pl.DataFrame(
                {
                    "pool_id": ["test-pool-1"],
                    "protocol_slug": ["test-protocol"],
                    "chain": ["ethereum"],
                    "symbol": ["TEST"],
                }
            )
            mock_tvl.return_value = pl.DataFrame(
                {
                    "pool_id": ["test-pool-1"],
                    "timestamp": [test_date],
                    "tvl_usd": [1000.0],
                    "apy": [5.0],
                    "apy_base": [2.0],
                    "apy_reward": [3.0],
                }
            )
            mock_dims.return_value = pl.DataFrame(
                {
                    "pool_id": ["test-pool-1"],
                    "pool_old": ["test-pool-1"],
                    "protocol_slug": ["test-protocol"],
                    "chain": ["ethereum"],
                    "symbol": ["TEST"],
                }
            )
            mock_filter.return_value = pl.DataFrame(
                {
                    "pool_id": ["test-pool-1"],
                    "pool_old": ["test-pool-1"],
                    "protocol_slug": ["test-protocol"],
                    "chain": ["ethereum"],
                    "symbol": ["TEST"],
                }
            )
            mock_facts.return_value = pl.DataFrame(
                {
                    "timestamp": [test_date],
                    "pool_id": [b"0x1234567890abcdef"],
                    "pool_id_defillama": ["test-pool-1"],
                    "protocol_slug": ["test-protocol"],
                    "chain": ["ethereum"],
                    "symbol": ["TEST"],
                    "tvl_usd": [1000.0],
                    "apy": [5.0],
                    "apy_base": [2.0],
                    "apy_reward": [3.0],
                },
                schema=HISTORICAL_FACTS_SCHEMA,
            )
            mock_save.return_value = None

            # Test with explicit target_date
            success = pipeline.run_daily_update(target_date=test_date)
            assert (
                success == True
            ), "run_daily_update() should work with explicit target_date"

            # Verify that create_historical_facts was called with the correct target_date
            mock_facts.assert_called_once()
            call_args = mock_facts.call_args
            assert (
                call_args[0][2] == test_date
            ), f"Expected target_date {test_date}, got {call_args[0][2]}"

            print("‚úÖ run_daily_update() with explicit target_date works correctly")

        # Test 2: Should raise TypeError when no target_date provided
        try:
            pipeline.run_daily_update()  # No target_date provided
            assert (
                False
            ), "run_daily_update() should raise TypeError when no target_date provided"
        except TypeError as e:
            print(
                "‚úÖ run_daily_update() correctly raises TypeError when no target_date provided"
            )
            print(f"   - Error message: {e}")

        print("‚úÖ run_daily_update() target_date requirement test passed")
        print("   - Works with explicit target_date")
        print("   - Raises TypeError when no target_date provided")
        print("   - target_date is now required for clarity")

        return True

    except Exception as e:
        print(f"‚ùå run_daily_update() target_date requirement test failed: {e}")
        return False


def test_daily_update_date_redundancy_prevention():
    """
    Test that run_daily_update() prevents redundant processing of the same date

    This test validates that the daily update doesn't process the same date twice
    """
    print("\nüß™ Testing Daily Update Date Redundancy Prevention")
    print("=" * 60)

    try:
        # Create pipeline orchestrator in dry-run mode
        pipeline = PipelineOrchestrator(dry_run=True)

        # Test with yesterday's date
        yesterday = date.today() - timedelta(days=1)

        with patch(
            "src.orchestration.pipeline.fetch_raw_pools_data"
        ) as mock_pools, patch(
            "src.orchestration.pipeline.fetch_raw_tvl_data"
        ) as mock_tvl, patch(
            "src.orchestration.pipeline.create_pool_dimensions"
        ) as mock_dims, patch(
            "src.orchestration.pipeline.filter_pools_by_projects"
        ) as mock_filter, patch(
            "src.orchestration.pipeline.create_historical_facts"
        ) as mock_facts, patch(
            "src.orchestration.pipeline.save_transformed_data"
        ) as mock_save:

            # Configure mocks with proper schema
            mock_pools.return_value = pl.DataFrame(
                {
                    "pool_id": ["test-pool-1"],
                    "protocol_slug": ["test-protocol"],
                    "chain": ["ethereum"],
                    "symbol": ["TEST"],
                }
            )
            mock_tvl.return_value = pl.DataFrame(
                {
                    "pool_id": ["test-pool-1"],
                    "timestamp": [yesterday],
                    "tvl_usd": [1000.0],
                    "apy": [5.0],
                    "apy_base": [2.0],
                    "apy_reward": [3.0],
                }
            )
            mock_dims.return_value = pl.DataFrame(
                {
                    "pool_id": ["test-pool-1"],
                    "pool_old": ["test-pool-1"],
                    "protocol_slug": ["test-protocol"],
                    "chain": ["ethereum"],
                    "symbol": ["TEST"],
                }
            )
            mock_filter.return_value = pl.DataFrame(
                {
                    "pool_id": ["test-pool-1"],
                    "pool_old": ["test-pool-1"],
                    "protocol_slug": ["test-protocol"],
                    "chain": ["ethereum"],
                    "symbol": ["TEST"],
                }
            )
            mock_facts.return_value = pl.DataFrame(
                {
                    "timestamp": [yesterday],
                    "pool_id": [b"0x1234567890abcdef"],
                    "pool_id_defillama": ["test-pool-1"],
                    "protocol_slug": ["test-protocol"],
                    "chain": ["ethereum"],
                    "symbol": ["TEST"],
                    "tvl_usd": [1000.0],
                    "apy": [5.0],
                    "apy_base": [2.0],
                    "apy_reward": [3.0],
                },
                schema=HISTORICAL_FACTS_SCHEMA,
            )
            mock_save.return_value = None

            # Test 1: First run with yesterday's date
            success1 = pipeline.run_daily_update(target_date=yesterday)
            assert success1 == True, "First run should succeed"

            # Verify that create_historical_facts was called with yesterday's date
            mock_facts.assert_called_once()
            call_args = mock_facts.call_args
            assert (
                call_args[0][2] == yesterday
            ), f"Expected yesterday's date {yesterday}, got {call_args[0][2]}"

            print("‚úÖ First run with yesterday's date succeeded")

            # Test 2: Second run with the same date (should be handled by duplicate detection in append_daily_facts)
            # Reset the mock to track the second call
            mock_facts.reset_mock()

            success2 = pipeline.run_daily_update(target_date=yesterday)
            assert (
                success2 == True
            ), "Second run should also succeed (duplicate detection handled by append_daily_facts)"

            # The create_historical_facts should be called again (duplicate detection happens in append_daily_facts)
            mock_facts.assert_called_once()
            call_args = mock_facts.call_args
            assert (
                call_args[0][2] == yesterday
            ), f"Expected yesterday's date {yesterday}, got {call_args[0][2]}"

            print(
                "‚úÖ Second run with same date succeeded (duplicate detection handled downstream)"
            )

        print("‚úÖ Daily update date redundancy prevention test passed")
        print("   - Multiple runs with same date are handled gracefully")
        print("   - Duplicate detection is handled by append_daily_facts() method")
        print("   - No data corruption occurs with repeated processing")

        return True

    except Exception as e:
        print(f"‚ùå Daily update date redundancy prevention test failed: {e}")
        return False


def main():
    """Run all pipeline unit tests"""

    print("üß™ Running Pipeline Unit Tests")
    print("=" * 70)
    print("These tests validate specific pipeline methods to avoid previous issues")
    print()

    # Run tests
    tests = [
        test_github_actions_initial_load_workflow,
        test_run_initial_load_produces_historical_facts,
        test_run_daily_update_requires_target_date,
        test_daily_update_date_redundancy_prevention,
    ]

    passed = 0
    total = len(tests)

    for test in tests:
        try:
            if test():
                passed += 1
            else:
                print(f"‚ùå Test {test.__name__} failed")
        except Exception as e:
            print(f"‚ùå Test {test.__name__} failed with exception: {e}")

    print(f"\nüìä Test Results: {passed}/{total} tests passed")

    if passed == total:
        print("üéâ All pipeline unit tests passed!")
        return True
    else:
        print("‚ùå Some tests failed. Please review the issues above.")
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
