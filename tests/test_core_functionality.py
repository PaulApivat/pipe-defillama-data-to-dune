#!/usr/bin/env python3
"""
Core Functionality Tests - Simplified

These tests focus on the essential working functions that need to be preserved
during refactoring. Based on the proven working behavior from the characterization.
"""

import polars as pl
import os
import sys
from datetime import date

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.fetch_tvl import fetch_tvl_functional
from scripts.fetch_current_state import fetch_current_state
from src.datasources.defillama.yieldpools.historical_tvl import YieldPoolsTVLFact
from src.coreutils.dune_uploader import DuneUploader
from dotenv import load_dotenv

load_dotenv()


def test_core_tvl_fetch():
    """Test the core TVL fetch functionality"""
    print("üß™ Testing core TVL fetch...")
    
    tvl_data = fetch_tvl_functional()
    
    # Verify we get the expected volume
    assert tvl_data.df.height > 400_000, f"Expected >400K records, got {tvl_data.df.height}"
    
    # Verify schema
    expected_columns = ["timestamp", "tvl_usd", "apy", "apy_base", "apy_reward", "pool_id"]
    for col in expected_columns:
        assert col in tvl_data.df.columns, f"Missing column: {col}"
    
    print(f"‚úÖ TVL fetch: {tvl_data.df.height:,} records")
    return tvl_data


def test_core_scd2_fetch():
    """Test the core SCD2 dimensions fetch functionality"""
    print("üß™ Testing core SCD2 fetch...")
    
    current_state, scd2_df = fetch_current_state()
    
    # Verify we get the expected volume
    assert scd2_df.height > 1_000, f"Expected >1K records, got {scd2_df.height}"
    
    # Verify SCD2 fields
    scd2_fields = ["pool_id", "valid_from", "valid_to", "is_current", "attrib_hash", "is_active"]
    for field in scd2_fields:
        assert field in scd2_df.columns, f"Missing SCD2 field: {field}"
    
    print(f"‚úÖ SCD2 fetch: {scd2_df.height:,} records")
    return scd2_df


def test_core_historical_facts_creation():
    """Test the core historical facts creation functionality"""
    print("üß™ Testing core historical facts creation...")
    
    # Load existing data (assuming it exists from previous runs)
    tvl_file = "output/tvl_data_2025-09-23.parquet"
    scd2_file = "output/pool_dim_scd2.parquet"
    
    if not os.path.exists(tvl_file):
        print(f"‚ö†Ô∏è  TVL file not found: {tvl_file}")
        return None
    
    if not os.path.exists(scd2_file):
        print(f"‚ö†Ô∏è  SCD2 file not found: {scd2_file}")
        return None
    
    # Load data
    tvl_data = pl.read_parquet(tvl_file)
    scd2_df = pl.read_parquet(scd2_file)
    
    # Create historical facts
    tvl_instance = YieldPoolsTVLFact(tvl_data)
    historical_facts = tvl_instance.create_historical_facts(scd2_df)
    
    # Verify we get the expected volume
    assert historical_facts.height > 400_000, f"Expected >400K records, got {historical_facts.height}"
    
    # Verify join worked (all TVL records should have dimension data)
    assert historical_facts.height == tvl_data.height, f"Join failed: {historical_facts.height} != {tvl_data.height}"
    
    print(f"‚úÖ Historical facts: {historical_facts.height:,} records")
    return historical_facts


def test_core_dune_upload():
    """Test the core Dune upload functionality"""
    print("üß™ Testing core Dune upload...")
    
    try:
        # Load historical facts
        historical_facts_file = "output/historical_facts_2025-09-23.parquet"
        if not os.path.exists(historical_facts_file):
            print(f"‚ö†Ô∏è  Historical facts file not found: {historical_facts_file}")
            return False
        
        historical_facts = pl.read_parquet(historical_facts_file)
        
        # Test Dune upload with sample data
        dune_uploader = DuneUploader()
        sample_data = historical_facts.head(5)
        dune_uploader.upload_historical_facts_data(sample_data)
        
        print("‚úÖ Dune upload: 5 sample records uploaded")
        return True
        
    except Exception as e:
        print(f"‚ùå Dune upload failed: {e}")
        return False


def test_core_pipeline_integration():
    """Test the core pipeline integration"""
    print("üß™ Testing core pipeline integration...")
    
    # This is the minimal working pipeline from your proof
    try:
        # Load historical facts (the final output)
        historical_facts_file = "output/historical_facts_2025-09-23.parquet"
        if not os.path.exists(historical_facts_file):
            print(f"‚ö†Ô∏è  Historical facts file not found: {historical_facts_file}")
            return False
        
        historical_facts = pl.read_parquet(historical_facts_file)
        
        # Verify the data matches your proven working state
        assert historical_facts.height == 496_662, f"Expected 496,662 records, got {historical_facts.height}"
        
        # Verify date range
        timestamps = historical_facts.select(pl.col("timestamp")).to_series().to_list()
        min_date = min(timestamps)
        max_date = max(timestamps)
        
        assert min_date == "2022-02-11", f"Expected min_date 2022-02-11, got {min_date}"
        assert max_date == "2025-09-22", f"Expected max_date 2025-09-22, got {max_date}"
        
        # Verify SCD2 fields are present
        scd2_fields = ["valid_from", "valid_to", "is_current", "attrib_hash", "is_active"]
        for field in scd2_fields:
            assert field in historical_facts.columns, f"Missing SCD2 field: {field}"
        
        print(f"‚úÖ Pipeline integration: {historical_facts.height:,} records, {min_date} to {max_date}")
        return True
        
    except Exception as e:
        print(f"‚ùå Pipeline integration failed: {e}")
        return False


def run_core_tests():
    """Run all core functionality tests"""
    print("üß™ RUNNING CORE FUNCTIONALITY TESTS")
    print("=" * 50)
    
    results = {}
    
    # Test 1: TVL Fetch
    try:
        results["tvl"] = test_core_tvl_fetch()
        print("‚úÖ TVL Fetch - PASSED")
    except Exception as e:
        print(f"‚ùå TVL Fetch - FAILED: {e}")
        results["tvl_error"] = str(e)
    
    # Test 2: SCD2 Fetch
    try:
        results["scd2"] = test_core_scd2_fetch()
        print("‚úÖ SCD2 Fetch - PASSED")
    except Exception as e:
        print(f"‚ùå SCD2 Fetch - FAILED: {e}")
        results["scd2_error"] = str(e)
    
    # Test 3: Historical Facts Creation
    try:
        results["historical_facts"] = test_core_historical_facts_creation()
        print("‚úÖ Historical Facts Creation - PASSED")
    except Exception as e:
        print(f"‚ùå Historical Facts Creation - FAILED: {e}")
        results["historical_facts_error"] = str(e)
    
    # Test 4: Dune Upload
    try:
        results["dune"] = test_core_dune_upload()
        print("‚úÖ Dune Upload - PASSED")
    except Exception as e:
        print(f"‚ùå Dune Upload - FAILED: {e}")
        results["dune_error"] = str(e)
    
    # Test 5: Pipeline Integration
    try:
        results["integration"] = test_core_pipeline_integration()
        print("‚úÖ Pipeline Integration - PASSED")
    except Exception as e:
        print(f"‚ùå Pipeline Integration - FAILED: {e}")
        results["integration_error"] = str(e)
    
    # Summary
    print("\nüéØ CORE FUNCTIONALITY TEST SUMMARY")
    print("=" * 50)
    
    passed_tests = 0
    total_tests = 5
    
    test_names = ["tvl", "scd2", "historical_facts", "dune", "integration"]
    for test_name in test_names:
        if f"{test_name}_error" not in results:
            passed_tests += 1
            print(f"‚úÖ {test_name.replace('_', ' ').title()}")
        else:
            print(f"‚ùå {test_name.replace('_', ' ').title()}")
    
    print(f"\nüìä Overall: {passed_tests}/{total_tests} tests passed")
    
    if passed_tests == total_tests:
        print("üéâ ALL CORE FUNCTIONALITY TESTS PASSED!")
        print("   The pipeline is working correctly and ready for refactoring.")
    else:
        print("‚ö†Ô∏è  Some tests failed. Fix issues before refactoring.")
    
    return results


if __name__ == "__main__":
    results = run_core_tests()
